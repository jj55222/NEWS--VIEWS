{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# FOIA-Free Content Pipeline v2 — Colab Runner\n\nTwo-lane architecture:\n- **Lane A** (Candidates): ingest → enrich → triage → corroborate → package → render\n- **Lane B** (Leads/Hunt): discover → hunt → verify → package → render\n\nSet your API keys below, then run each stage.\n- `OPENROUTER_API_KEY` — required for LLM scoring\n- `BRAVE_API_KEY` — required for artifact hunt + corroboration\n- `YOUTUBE_API_KEY` — optional (RSS fallback if not set)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. Install dependencies\n!pip install -q pyyaml python-dotenv openai requests feedparser beautifulsoup4 yt-dlp cloudscraper\n!apt-get -qq install -y ffmpeg"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2. Clone repo and set up Python path\nimport os, sys, subprocess\n\nREPO_URL = 'https://github.com/jj55222/NEWS--VIEWS.git'\nBRANCH = 'claude/foia-free-content-pipeline-qlbzp'\nREPO_DIR = '/content/NEWS--VIEWS'\n\n# Always reset to a safe directory first (handles deleted-cwd edge case)\nos.chdir('/content')\n\nif os.path.isdir(REPO_DIR):\n    # Repo exists — make sure we're on the right branch and pull latest\n    os.chdir(REPO_DIR)\n    subprocess.run(['git', 'fetch', 'origin', BRANCH], check=False)\n    subprocess.run(['git', 'checkout', BRANCH], check=False)\n    subprocess.run(['git', 'pull', 'origin', BRANCH], check=False)\nelse:\n    !git clone -b {BRANCH} {REPO_URL} {REPO_DIR}\n    os.chdir(REPO_DIR)\n\n# Add to Python path so \"from scripts...\" imports work\nif REPO_DIR not in sys.path:\n    sys.path.insert(0, REPO_DIR)\n\nprint('Working directory:', os.getcwd())\nprint('scripts/ exists:', os.path.isdir('scripts'))\nprint('config/ exists:', os.path.isdir('config'))\nif not os.path.isdir('scripts'):\n    print('\\n⚠ scripts/ missing — run this in a new cell, then re-run this cell:')\n    print('  !rm -rf /content/NEWS--VIEWS')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 3. Set API keys\nimport os\n\n# REQUIRED\nos.environ['OPENROUTER_API_KEY'] = ''  # Your OpenRouter API key\nos.environ['BRAVE_API_KEY'] = ''       # Your Brave Search API key (https://brave.com/search/api/)\n\n# OPTIONAL (YouTube RSS fallback if not set)\nos.environ['YOUTUBE_API_KEY'] = ''     # Your YouTube Data API v3 key\n\nkeys_set = bool(os.environ.get('OPENROUTER_API_KEY')) and bool(os.environ.get('BRAVE_API_KEY'))\nprint('API keys configured.' if keys_set else 'WARNING: Set OPENROUTER_API_KEY and BRAVE_API_KEY above!')\nif not os.environ.get('YOUTUBE_API_KEY'):\n    print('Note: YOUTUBE_API_KEY not set — YouTube ingest will use RSS fallback (no duration/view data).')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4. Initialize database\nimport sys, os\n\n# Safety net: ensure project root is on path even if cell 2 was skipped/stale\nREPO_DIR = '/content/NEWS--VIEWS'\nif os.path.isdir(REPO_DIR):\n    os.chdir(REPO_DIR)\n    if REPO_DIR not in sys.path:\n        sys.path.insert(0, REPO_DIR)\n\nfrom scripts.db import init_db\nfrom scripts.config_loader import ensure_dirs, load_policy, load_sources\n\nensure_dirs()\ninit_db()\n\npolicy = load_policy()\nsources = load_sources()\nenabled = [s for s in sources if s.get(\"enabled\")]\nprimary = [s for s in enabled if s.get(\"source_class\") == \"primary\"]\nsecondary = [s for s in enabled if s.get(\"source_class\") == \"secondary\"]\ndiscovery = [s for s in enabled if s.get(\"source_class\") == \"discovery_only\"]\n\nprint(f'Policy: {len(policy)} sections')\nprint(f'Sources: {len(sources)} total ({len(enabled)} enabled)')\nprint(f'  primary: {len(primary)} | secondary: {len(secondary)} | discovery_only: {len(discovery)}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ═══ LANE A: Candidate Flow ═══════════════════════════════════════════════\n\n# 5a. INGEST — Pull new candidates (YouTube + RSS + pages)\nfrom scripts.run_pipeline import run_ingest\n\ningest_results = run_ingest(days=7, dry_run=False)\ningest_results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b. ENRICH — Add transcripts and entities\n",
    "from scripts.run_pipeline import run_enrich\n",
    "\n",
    "enrich_results = run_enrich(limit=200, dry_run=False)\n",
    "enrich_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5c. TRIAGE — Score and classify candidates\n",
    "from scripts.run_pipeline import run_triage\n",
    "\n",
    "triage_results = run_triage(limit=200, dry_run=False)\n",
    "triage_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 6. Review PASS candidates (Lane A)\nfrom scripts.db import get_connection, get_candidates\nimport json\n\nconn = get_connection()\npass_candidates = get_candidates(conn, status='PASS', limit=50)\nprint(f'PASS candidates: {len(pass_candidates)}')\nprint()\n\nfor i, c in enumerate(pass_candidates[:20], 1):\n    sc = c.get('source_class', '?')\n    print(f'{i:2d}. [{c[\"triage_score\"]:3d}] [{sc}] {c[\"title\"][:75]}')\n    print(f'    Type: {c[\"incident_type\"]} | URL: {c[\"url\"]}')\n    print(f'    Reason: {(c.get(\"triage_rationale\") or \"\")[:100]}')\n    print()\n\nconn.close()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. CORROBORATE — Gather supporting sources for PASS candidates\n",
    "from scripts.run_pipeline import run_corroborate\n",
    "\n",
    "corr_results = run_corroborate(limit=20, dry_run=False)\n",
    "corr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. PACKAGE — Build case bundles (timeline, narration, shorts plan)\n",
    "from scripts.run_pipeline import run_package\n",
    "\n",
    "package_results = run_package(limit=5, dry_run=False)\n",
    "package_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. RENDER — Download, cut, caption, export\n",
    "from scripts.run_pipeline import run_render\n",
    "\n",
    "render_results = run_render(limit=3, dry_run=False)\n",
    "render_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ═══ LANE B: Lead/Artifact Flow (v2) ═════════════════════════════════════\n\n# 10a. DISCOVER — RSS + pages → case_leads with hook scoring\nfrom scripts.run_pipeline import run_discover\n\ndiscover_results = run_discover(days=7, dry_run=False)\ndiscover_results"
  },
  {
   "cell_type": "code",
   "source": "# 10b. HUNT — Brave Search for primary artifacts (bodycam, dashcam, court, docs)\nfrom scripts.run_pipeline import run_hunt\n\nhunt_results = run_hunt(min_hook=70, limit=50, dry_run=False)\nhunt_results",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 12. Pipeline stats (both lanes)\nfrom scripts.db import get_connection\n\nconn = get_connection()\nprint('=== Pipeline Stats ===')\nfor table in ['candidates', 'case_leads', 'artifacts', 'case_bundles', 'cases', 'corroboration_sources']:\n    try:\n        count = conn.execute(f'SELECT COUNT(*) FROM {table}').fetchone()[0]\n        if count > 0:\n            print(f'  {table}: {count} rows')\n    except Exception:\n        pass\n\nprint()\nprint('Lane A — Triage distribution:')\nfor status in ['NEW', 'PASS', 'MAYBE', 'KILL']:\n    count = conn.execute(\n        'SELECT COUNT(*) FROM candidates WHERE triage_status = ?', (status,)\n    ).fetchone()[0]\n    if count > 0:\n        print(f'  {status}: {count}')\n\nprint()\nprint('Lane B — Lead status distribution:')\nfor status in ['NEW', 'HUNTING', 'ARTIFACT_FOUND', 'NO_ARTIFACT', 'KILL']:\n    count = conn.execute(\n        'SELECT COUNT(*) FROM case_leads WHERE status = ?', (status,)\n    ).fetchone()[0]\n    if count > 0:\n        print(f'  {status}: {count}')\n\nprint()\nprint('Bundle status distribution:')\nfor status in ['APPROVED', 'PACKAGED', 'RENDERED', 'READY_TO_PUBLISH']:\n    count = conn.execute(\n        'SELECT COUNT(*) FROM case_bundles WHERE status = ?', (status,)\n    ).fetchone()[0]\n    if count > 0:\n        print(f'  {status}: {count}')\n\nconn.close()"
  },
  {
   "cell_type": "code",
   "source": "# 10c. Review leads + artifacts\nfrom scripts.db import get_connection, get_leads, get_artifacts\n\nconn = get_connection()\n\n# Leads summary\nfor status in ['NEW', 'HUNTING', 'ARTIFACT_FOUND', 'NO_ARTIFACT']:\n    leads = get_leads(conn, status=status)\n    if leads:\n        print(f'=== {status} leads: {len(leads)} ===')\n        for i, lead in enumerate(leads[:10], 1):\n            print(f'  {i:2d}. [{lead[\"hook_score\"]:3d}] {lead[\"title\"][:70]}')\n            if status == 'ARTIFACT_FOUND':\n                arts = get_artifacts(conn, lead['lead_id'])\n                primary = [a for a in arts if a.get('source_class') == 'primary']\n                print(f'      Artifacts: {len(arts)} total, {len(primary)} primary')\n        print()\n\nconn.close()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 10d. VERIFY + PACKAGE — Corroborate leads and build case bundles\nfrom scripts.run_pipeline import run_verify, run_package_v2\n\nverify_results = run_verify(limit=20, dry_run=False)\nprint('Verify:', verify_results)\n\npackage_v2_results = run_package_v2(limit=10, dry_run=False)\nprint('Package v2:', package_v2_results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 11. Missed Opportunity Report\nfrom scripts.run_pipeline import run_report\n\nreport_summary = run_report(top_n=30)\nreport_summary",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# FULL PIPELINE (single command)\n# Uncomment the lane you want to run:\n\n# Lane A only (v1 candidate flow):\n# from scripts.run_pipeline import run_pipeline\n# results = run_pipeline(lane='a', days=7, dry_run=False)\n# results\n\n# Lane B only (v2 lead/artifact flow):\n# from scripts.run_pipeline import run_pipeline\n# results = run_pipeline(lane='b', days=7, dry_run=False)\n# results\n\n# Both lanes:\n# from scripts.run_pipeline import run_pipeline\n# results = run_pipeline(days=7, dry_run=False)\n# results",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}